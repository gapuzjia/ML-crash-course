{"cells":[{"cell_type":"markdown","metadata":{"id":"ev7mKEvgbYcZ"},"source":["# Lab 10: Fully Connected Neural Networks\n","\n","In this assignment, we will learn fully connected neural network.\n"]},{"cell_type":"markdown","metadata":{"id":"vwZwfFQYbYcc"},"source":["## 1. Example"]},{"cell_type":"markdown","metadata":{"id":"WHnoQz3MbYcd"},"source":["This assignement should be run on Google Colab where you can use free GPU to accelerate the computation. Please refer to our slides to set up GPU."]},{"cell_type":"markdown","metadata":{"id":"u4Rte_TebYce"},"source":["### 1. Install Pytorch"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5675,"status":"ok","timestamp":1718129417652,"user":{"displayName":"Jia Gapuz","userId":"08065593811390920067"},"user_tz":240},"id":"5YjZrt3dW4SB","outputId":"8f1f31cf-9508-4ac5-97b7-82f9c4e28cd4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}],"source":["!pip3 install torch torchvision   # install pytorch"]},{"cell_type":"markdown","metadata":{"id":"cqu1YdX2bYch"},"source":["### 2. Check GPU"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1718129417652,"user":{"displayName":"Jia Gapuz","userId":"08065593811390920067"},"user_tz":240},"id":"KH1pKuAAXqzd","outputId":"d24413ad-85b3-4640-a047-fc263fbd3361"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Jun 11 18:10:16 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   52C    P0              26W /  70W |    169MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!/opt/bin/nvidia-smi  #show GPU"]},{"cell_type":"markdown","metadata":{"id":"bqmII1sAbYcj"},"source":["### 3. Mount to google drive (optional)"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2984,"status":"ok","timestamp":1718129420630,"user":{"displayName":"Jia Gapuz","userId":"08065593811390920067"},"user_tz":240},"id":"HdWuV25Qb-TM","outputId":"cd3c18e9-fca5-4f30-800f-9dbad029ab1d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"_bAPbFl0bYck"},"source":["### 4. Code"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"-nVVA8yeXv9c","executionInfo":{"status":"ok","timestamp":1718129420631,"user_tz":240,"elapsed":9,"user":{"displayName":"Jia Gapuz","userId":"08065593811390920067"}}},"outputs":[],"source":["#Import Libraries\n","from __future__ import print_function\n","import argparse\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"eTnQJNfyhmaE","executionInfo":{"status":"ok","timestamp":1718129420631,"user_tz":240,"elapsed":8,"user":{"displayName":"Jia Gapuz","userId":"08065593811390920067"}}},"outputs":[],"source":["args={}\n","args['batch_size']=100\n","args['test_batch_size']=100\n","args['epochs']=10  #The number of Epochs is the number of times you go through the full dataset.\n","args['lr']=0.01 #Learning rate is how fast it will decend.\n","args['log_interval']=10"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1718129420631,"user":{"displayName":"Jia Gapuz","userId":"08065593811390920067"},"user_tz":240},"id":"4aMP_CHcX7jJ","outputId":"7ca90ea4-4a4e-4484-c0c2-94f2c1c92e47"},"outputs":[{"output_type":"stream","name":"stdout","text":["Net(\n","  (fc1): Linear(in_features=784, out_features=256, bias=True)\n","  (fc2): Linear(in_features=256, out_features=128, bias=True)\n","  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",")\n"]}],"source":["# build an mlp\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","\n","        self.fc1 = nn.Linear(28*28, 256)   # linear layer (784 -> 256)\n","        self.fc2 = nn.Linear(256,128)  # linear layer (256 -> 128)\n","        self.fc3 = nn.Linear(128,10)  # linear layer (128 -> 10)\n","\n","    def forward(self, x):\n","        h0 = x.view(-1,28*28) #input layer\n","        h1 = F.relu(self.fc1(h0)) # hidden layer 1\n","        h2 = F.relu(self.fc2(h1)) # hidden layer 2\n","        h3 = self.fc3(h2) # output layer\n","\n","        return h3\n","\n","model = Net()\n","model.cuda() # put the model on GPU\n","print(model)"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"0U9yHGoTZi3e","executionInfo":{"status":"ok","timestamp":1718129420631,"user_tz":240,"elapsed":6,"user":{"displayName":"Jia Gapuz","userId":"08065593811390920067"}}},"outputs":[],"source":["# loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","# optimizer\n","optimizer = torch.optim.SGD(model.parameters(),lr = args['lr'])"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"YvuVjPqPabRC","executionInfo":{"status":"ok","timestamp":1718129420631,"user_tz":240,"elapsed":6,"user":{"displayName":"Jia Gapuz","userId":"08065593811390920067"}}},"outputs":[],"source":["#load the data\n","train_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('./data', train=True, download=True,\n","                   transform=transforms.Compose([\n","                       transforms.ToTensor(),\n","                       transforms.Normalize((0.1307,), (0.3081,))\n","                   ])),\n","    batch_size=args['batch_size'], shuffle=True)\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('./data', train=False, transform=transforms.Compose([\n","                       transforms.ToTensor(),\n","                       transforms.Normalize((0.1307,), (0.3081,))\n","                   ])),\n","    batch_size=args['test_batch_size'], shuffle=False)"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"YopP0oK5ca2u","executionInfo":{"status":"ok","timestamp":1718129420631,"user_tz":240,"elapsed":5,"user":{"displayName":"Jia Gapuz","userId":"08065593811390920067"}}},"outputs":[],"source":["\n","def train(epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.cuda(), target.cuda()\n","\n","        output = model(data)\n","        loss = criterion(output, target)\n","\n","        # compute gradients\n","        optimizer.zero_grad()\n","        loss.backward()\n","\n","        #to do a one-step update on our parameter.\n","        optimizer.step()\n","\n","        #Print out the loss periodically.\n","        if batch_idx % args['log_interval'] == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"wLn-uP_sct-a","executionInfo":{"status":"ok","timestamp":1718129420631,"user_tz":240,"elapsed":5,"user":{"displayName":"Jia Gapuz","userId":"08065593811390920067"}}},"outputs":[],"source":["def test():\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    for data, target in test_loader:\n","        data, target = data.cuda(), target.cuda()\n","\n","        output = model(data)\n","        test_loss += criterion(output, target).item() # sum up batch loss\n","        pred = output.data.max(1, keepdim=True)[1]\n","        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":166466,"status":"ok","timestamp":1718129587092,"user":{"displayName":"Jia Gapuz","userId":"08065593811390920067"},"user_tz":240},"id":"FgF4zxVlc28U","outputId":"7ca8b761-c856-4361-cb04-728120cc2f1a","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.296016\n","Train Epoch: 1 [1000/60000 (2%)]\tLoss: 2.281802\n","Train Epoch: 1 [2000/60000 (3%)]\tLoss: 2.253489\n","Train Epoch: 1 [3000/60000 (5%)]\tLoss: 2.207014\n","Train Epoch: 1 [4000/60000 (7%)]\tLoss: 2.211993\n","Train Epoch: 1 [5000/60000 (8%)]\tLoss: 2.167615\n","Train Epoch: 1 [6000/60000 (10%)]\tLoss: 2.109080\n","Train Epoch: 1 [7000/60000 (12%)]\tLoss: 2.093056\n","Train Epoch: 1 [8000/60000 (13%)]\tLoss: 2.019979\n","Train Epoch: 1 [9000/60000 (15%)]\tLoss: 1.966872\n","Train Epoch: 1 [10000/60000 (17%)]\tLoss: 1.890509\n","Train Epoch: 1 [11000/60000 (18%)]\tLoss: 1.836447\n","Train Epoch: 1 [12000/60000 (20%)]\tLoss: 1.733401\n","Train Epoch: 1 [13000/60000 (22%)]\tLoss: 1.645925\n","Train Epoch: 1 [14000/60000 (23%)]\tLoss: 1.626742\n","Train Epoch: 1 [15000/60000 (25%)]\tLoss: 1.568094\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.373687\n","Train Epoch: 1 [17000/60000 (28%)]\tLoss: 1.311444\n","Train Epoch: 1 [18000/60000 (30%)]\tLoss: 1.284683\n","Train Epoch: 1 [19000/60000 (32%)]\tLoss: 1.109026\n","Train Epoch: 1 [20000/60000 (33%)]\tLoss: 1.186035\n","Train Epoch: 1 [21000/60000 (35%)]\tLoss: 1.082857\n","Train Epoch: 1 [22000/60000 (37%)]\tLoss: 1.132145\n","Train Epoch: 1 [23000/60000 (38%)]\tLoss: 0.888983\n","Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.983830\n","Train Epoch: 1 [25000/60000 (42%)]\tLoss: 0.887279\n","Train Epoch: 1 [26000/60000 (43%)]\tLoss: 0.728249\n","Train Epoch: 1 [27000/60000 (45%)]\tLoss: 0.645432\n","Train Epoch: 1 [28000/60000 (47%)]\tLoss: 0.733633\n","Train Epoch: 1 [29000/60000 (48%)]\tLoss: 0.688891\n","Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.700645\n","Train Epoch: 1 [31000/60000 (52%)]\tLoss: 0.714629\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.664395\n","Train Epoch: 1 [33000/60000 (55%)]\tLoss: 0.574565\n","Train Epoch: 1 [34000/60000 (57%)]\tLoss: 0.579004\n","Train Epoch: 1 [35000/60000 (58%)]\tLoss: 0.545005\n","Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.557793\n","Train Epoch: 1 [37000/60000 (62%)]\tLoss: 0.568522\n","Train Epoch: 1 [38000/60000 (63%)]\tLoss: 0.601759\n","Train Epoch: 1 [39000/60000 (65%)]\tLoss: 0.531628\n","Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.596831\n","Train Epoch: 1 [41000/60000 (68%)]\tLoss: 0.569273\n","Train Epoch: 1 [42000/60000 (70%)]\tLoss: 0.498715\n","Train Epoch: 1 [43000/60000 (72%)]\tLoss: 0.606484\n","Train Epoch: 1 [44000/60000 (73%)]\tLoss: 0.536642\n","Train Epoch: 1 [45000/60000 (75%)]\tLoss: 0.555404\n","Train Epoch: 1 [46000/60000 (77%)]\tLoss: 0.473045\n","Train Epoch: 1 [47000/60000 (78%)]\tLoss: 0.571996\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.496531\n","Train Epoch: 1 [49000/60000 (82%)]\tLoss: 0.414322\n","Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.446616\n","Train Epoch: 1 [51000/60000 (85%)]\tLoss: 0.482394\n","Train Epoch: 1 [52000/60000 (87%)]\tLoss: 0.456587\n","Train Epoch: 1 [53000/60000 (88%)]\tLoss: 0.394315\n","Train Epoch: 1 [54000/60000 (90%)]\tLoss: 0.475064\n","Train Epoch: 1 [55000/60000 (92%)]\tLoss: 0.400846\n","Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.429476\n","Train Epoch: 1 [57000/60000 (95%)]\tLoss: 0.371178\n","Train Epoch: 1 [58000/60000 (97%)]\tLoss: 0.355912\n","Train Epoch: 1 [59000/60000 (98%)]\tLoss: 0.535269\n","\n","Test set: Average loss: 0.0040, Accuracy: 8909/10000 (89%)\n","\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.431441\n","Train Epoch: 2 [1000/60000 (2%)]\tLoss: 0.401686\n","Train Epoch: 2 [2000/60000 (3%)]\tLoss: 0.492875\n","Train Epoch: 2 [3000/60000 (5%)]\tLoss: 0.526829\n","Train Epoch: 2 [4000/60000 (7%)]\tLoss: 0.248851\n","Train Epoch: 2 [5000/60000 (8%)]\tLoss: 0.479870\n","Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.323820\n","Train Epoch: 2 [7000/60000 (12%)]\tLoss: 0.272010\n","Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.441251\n","Train Epoch: 2 [9000/60000 (15%)]\tLoss: 0.351584\n","Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.323977\n","Train Epoch: 2 [11000/60000 (18%)]\tLoss: 0.316113\n","Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.408178\n","Train Epoch: 2 [13000/60000 (22%)]\tLoss: 0.395227\n","Train Epoch: 2 [14000/60000 (23%)]\tLoss: 0.403578\n","Train Epoch: 2 [15000/60000 (25%)]\tLoss: 0.357111\n","Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.379538\n","Train Epoch: 2 [17000/60000 (28%)]\tLoss: 0.364763\n","Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.288023\n","Train Epoch: 2 [19000/60000 (32%)]\tLoss: 0.366923\n","Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.307466\n","Train Epoch: 2 [21000/60000 (35%)]\tLoss: 0.290919\n","Train Epoch: 2 [22000/60000 (37%)]\tLoss: 0.218557\n","Train Epoch: 2 [23000/60000 (38%)]\tLoss: 0.531886\n","Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.291657\n","Train Epoch: 2 [25000/60000 (42%)]\tLoss: 0.429244\n","Train Epoch: 2 [26000/60000 (43%)]\tLoss: 0.459105\n","Train Epoch: 2 [27000/60000 (45%)]\tLoss: 0.301459\n","Train Epoch: 2 [28000/60000 (47%)]\tLoss: 0.358023\n","Train Epoch: 2 [29000/60000 (48%)]\tLoss: 0.268800\n","Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.261098\n","Train Epoch: 2 [31000/60000 (52%)]\tLoss: 0.300110\n","Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.425846\n","Train Epoch: 2 [33000/60000 (55%)]\tLoss: 0.540746\n","Train Epoch: 2 [34000/60000 (57%)]\tLoss: 0.349359\n","Train Epoch: 2 [35000/60000 (58%)]\tLoss: 0.389586\n","Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.342814\n","Train Epoch: 2 [37000/60000 (62%)]\tLoss: 0.293664\n","Train Epoch: 2 [38000/60000 (63%)]\tLoss: 0.367152\n","Train Epoch: 2 [39000/60000 (65%)]\tLoss: 0.375175\n","Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.409480\n","Train Epoch: 2 [41000/60000 (68%)]\tLoss: 0.475769\n","Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.450257\n","Train Epoch: 2 [43000/60000 (72%)]\tLoss: 0.291727\n","Train Epoch: 2 [44000/60000 (73%)]\tLoss: 0.220445\n","Train Epoch: 2 [45000/60000 (75%)]\tLoss: 0.409568\n","Train Epoch: 2 [46000/60000 (77%)]\tLoss: 0.313700\n","Train Epoch: 2 [47000/60000 (78%)]\tLoss: 0.253061\n","Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.359951\n","Train Epoch: 2 [49000/60000 (82%)]\tLoss: 0.259826\n","Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.495658\n","Train Epoch: 2 [51000/60000 (85%)]\tLoss: 0.402134\n","Train Epoch: 2 [52000/60000 (87%)]\tLoss: 0.218339\n","Train Epoch: 2 [53000/60000 (88%)]\tLoss: 0.331827\n","Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.209297\n","Train Epoch: 2 [55000/60000 (92%)]\tLoss: 0.301874\n","Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.349610\n","Train Epoch: 2 [57000/60000 (95%)]\tLoss: 0.321468\n","Train Epoch: 2 [58000/60000 (97%)]\tLoss: 0.252669\n","Train Epoch: 2 [59000/60000 (98%)]\tLoss: 0.417793\n","\n","Test set: Average loss: 0.0030, Accuracy: 9115/10000 (91%)\n","\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.400385\n","Train Epoch: 3 [1000/60000 (2%)]\tLoss: 0.268759\n","Train Epoch: 3 [2000/60000 (3%)]\tLoss: 0.224591\n","Train Epoch: 3 [3000/60000 (5%)]\tLoss: 0.214064\n","Train Epoch: 3 [4000/60000 (7%)]\tLoss: 0.287952\n","Train Epoch: 3 [5000/60000 (8%)]\tLoss: 0.353938\n","Train Epoch: 3 [6000/60000 (10%)]\tLoss: 0.228017\n","Train Epoch: 3 [7000/60000 (12%)]\tLoss: 0.425447\n","Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.340904\n","Train Epoch: 3 [9000/60000 (15%)]\tLoss: 0.331626\n","Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.246427\n","Train Epoch: 3 [11000/60000 (18%)]\tLoss: 0.302894\n","Train Epoch: 3 [12000/60000 (20%)]\tLoss: 0.350051\n","Train Epoch: 3 [13000/60000 (22%)]\tLoss: 0.297840\n","Train Epoch: 3 [14000/60000 (23%)]\tLoss: 0.271858\n","Train Epoch: 3 [15000/60000 (25%)]\tLoss: 0.234047\n","Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.369264\n","Train Epoch: 3 [17000/60000 (28%)]\tLoss: 0.250162\n","Train Epoch: 3 [18000/60000 (30%)]\tLoss: 0.239543\n","Train Epoch: 3 [19000/60000 (32%)]\tLoss: 0.258256\n","Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.202769\n","Train Epoch: 3 [21000/60000 (35%)]\tLoss: 0.345128\n","Train Epoch: 3 [22000/60000 (37%)]\tLoss: 0.325753\n","Train Epoch: 3 [23000/60000 (38%)]\tLoss: 0.287017\n","Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.416512\n","Train Epoch: 3 [25000/60000 (42%)]\tLoss: 0.321062\n","Train Epoch: 3 [26000/60000 (43%)]\tLoss: 0.309730\n","Train Epoch: 3 [27000/60000 (45%)]\tLoss: 0.270107\n","Train Epoch: 3 [28000/60000 (47%)]\tLoss: 0.245117\n","Train Epoch: 3 [29000/60000 (48%)]\tLoss: 0.321967\n","Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.361260\n","Train Epoch: 3 [31000/60000 (52%)]\tLoss: 0.252802\n","Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.566918\n","Train Epoch: 3 [33000/60000 (55%)]\tLoss: 0.312146\n","Train Epoch: 3 [34000/60000 (57%)]\tLoss: 0.494420\n","Train Epoch: 3 [35000/60000 (58%)]\tLoss: 0.251868\n","Train Epoch: 3 [36000/60000 (60%)]\tLoss: 0.267011\n","Train Epoch: 3 [37000/60000 (62%)]\tLoss: 0.195699\n","Train Epoch: 3 [38000/60000 (63%)]\tLoss: 0.194751\n","Train Epoch: 3 [39000/60000 (65%)]\tLoss: 0.365543\n","Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.222578\n","Train Epoch: 3 [41000/60000 (68%)]\tLoss: 0.401550\n","Train Epoch: 3 [42000/60000 (70%)]\tLoss: 0.297079\n","Train Epoch: 3 [43000/60000 (72%)]\tLoss: 0.166127\n","Train Epoch: 3 [44000/60000 (73%)]\tLoss: 0.292810\n","Train Epoch: 3 [45000/60000 (75%)]\tLoss: 0.325301\n","Train Epoch: 3 [46000/60000 (77%)]\tLoss: 0.373701\n","Train Epoch: 3 [47000/60000 (78%)]\tLoss: 0.251682\n","Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.316609\n","Train Epoch: 3 [49000/60000 (82%)]\tLoss: 0.315910\n","Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.229546\n","Train Epoch: 3 [51000/60000 (85%)]\tLoss: 0.261624\n","Train Epoch: 3 [52000/60000 (87%)]\tLoss: 0.343805\n","Train Epoch: 3 [53000/60000 (88%)]\tLoss: 0.290102\n","Train Epoch: 3 [54000/60000 (90%)]\tLoss: 0.182927\n","Train Epoch: 3 [55000/60000 (92%)]\tLoss: 0.465447\n","Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.407016\n","Train Epoch: 3 [57000/60000 (95%)]\tLoss: 0.262988\n","Train Epoch: 3 [58000/60000 (97%)]\tLoss: 0.250714\n","Train Epoch: 3 [59000/60000 (98%)]\tLoss: 0.262604\n","\n","Test set: Average loss: 0.0027, Accuracy: 9222/10000 (92%)\n","\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.205477\n","Train Epoch: 4 [1000/60000 (2%)]\tLoss: 0.239766\n","Train Epoch: 4 [2000/60000 (3%)]\tLoss: 0.268548\n","Train Epoch: 4 [3000/60000 (5%)]\tLoss: 0.293542\n","Train Epoch: 4 [4000/60000 (7%)]\tLoss: 0.229764\n","Train Epoch: 4 [5000/60000 (8%)]\tLoss: 0.232844\n","Train Epoch: 4 [6000/60000 (10%)]\tLoss: 0.248483\n","Train Epoch: 4 [7000/60000 (12%)]\tLoss: 0.227122\n","Train Epoch: 4 [8000/60000 (13%)]\tLoss: 0.198966\n","Train Epoch: 4 [9000/60000 (15%)]\tLoss: 0.297234\n","Train Epoch: 4 [10000/60000 (17%)]\tLoss: 0.259793\n","Train Epoch: 4 [11000/60000 (18%)]\tLoss: 0.274557\n","Train Epoch: 4 [12000/60000 (20%)]\tLoss: 0.210184\n","Train Epoch: 4 [13000/60000 (22%)]\tLoss: 0.274595\n","Train Epoch: 4 [14000/60000 (23%)]\tLoss: 0.249949\n","Train Epoch: 4 [15000/60000 (25%)]\tLoss: 0.331277\n","Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.286602\n","Train Epoch: 4 [17000/60000 (28%)]\tLoss: 0.295720\n","Train Epoch: 4 [18000/60000 (30%)]\tLoss: 0.325247\n","Train Epoch: 4 [19000/60000 (32%)]\tLoss: 0.271990\n","Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.466714\n","Train Epoch: 4 [21000/60000 (35%)]\tLoss: 0.261020\n","Train Epoch: 4 [22000/60000 (37%)]\tLoss: 0.209828\n","Train Epoch: 4 [23000/60000 (38%)]\tLoss: 0.351887\n","Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.287090\n","Train Epoch: 4 [25000/60000 (42%)]\tLoss: 0.435221\n","Train Epoch: 4 [26000/60000 (43%)]\tLoss: 0.178442\n","Train Epoch: 4 [27000/60000 (45%)]\tLoss: 0.190982\n","Train Epoch: 4 [28000/60000 (47%)]\tLoss: 0.199974\n","Train Epoch: 4 [29000/60000 (48%)]\tLoss: 0.254065\n","Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.198414\n","Train Epoch: 4 [31000/60000 (52%)]\tLoss: 0.254325\n","Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.330482\n","Train Epoch: 4 [33000/60000 (55%)]\tLoss: 0.291091\n","Train Epoch: 4 [34000/60000 (57%)]\tLoss: 0.173620\n","Train Epoch: 4 [35000/60000 (58%)]\tLoss: 0.288956\n","Train Epoch: 4 [36000/60000 (60%)]\tLoss: 0.161348\n","Train Epoch: 4 [37000/60000 (62%)]\tLoss: 0.317687\n","Train Epoch: 4 [38000/60000 (63%)]\tLoss: 0.199636\n","Train Epoch: 4 [39000/60000 (65%)]\tLoss: 0.245184\n","Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.356961\n","Train Epoch: 4 [41000/60000 (68%)]\tLoss: 0.168873\n","Train Epoch: 4 [42000/60000 (70%)]\tLoss: 0.238012\n","Train Epoch: 4 [43000/60000 (72%)]\tLoss: 0.292464\n","Train Epoch: 4 [44000/60000 (73%)]\tLoss: 0.187690\n","Train Epoch: 4 [45000/60000 (75%)]\tLoss: 0.261988\n","Train Epoch: 4 [46000/60000 (77%)]\tLoss: 0.159924\n","Train Epoch: 4 [47000/60000 (78%)]\tLoss: 0.284880\n","Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.144867\n","Train Epoch: 4 [49000/60000 (82%)]\tLoss: 0.264581\n","Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.254316\n","Train Epoch: 4 [51000/60000 (85%)]\tLoss: 0.256870\n","Train Epoch: 4 [52000/60000 (87%)]\tLoss: 0.204638\n","Train Epoch: 4 [53000/60000 (88%)]\tLoss: 0.227708\n","Train Epoch: 4 [54000/60000 (90%)]\tLoss: 0.254145\n","Train Epoch: 4 [55000/60000 (92%)]\tLoss: 0.255720\n","Train Epoch: 4 [56000/60000 (93%)]\tLoss: 0.249678\n","Train Epoch: 4 [57000/60000 (95%)]\tLoss: 0.354358\n","Train Epoch: 4 [58000/60000 (97%)]\tLoss: 0.166224\n","Train Epoch: 4 [59000/60000 (98%)]\tLoss: 0.251244\n","\n","Test set: Average loss: 0.0024, Accuracy: 9295/10000 (93%)\n","\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.244468\n","Train Epoch: 5 [1000/60000 (2%)]\tLoss: 0.282617\n","Train Epoch: 5 [2000/60000 (3%)]\tLoss: 0.262939\n","Train Epoch: 5 [3000/60000 (5%)]\tLoss: 0.372330\n","Train Epoch: 5 [4000/60000 (7%)]\tLoss: 0.292470\n","Train Epoch: 5 [5000/60000 (8%)]\tLoss: 0.208439\n","Train Epoch: 5 [6000/60000 (10%)]\tLoss: 0.212461\n","Train Epoch: 5 [7000/60000 (12%)]\tLoss: 0.277460\n","Train Epoch: 5 [8000/60000 (13%)]\tLoss: 0.305509\n","Train Epoch: 5 [9000/60000 (15%)]\tLoss: 0.142285\n","Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.157491\n","Train Epoch: 5 [11000/60000 (18%)]\tLoss: 0.115651\n","Train Epoch: 5 [12000/60000 (20%)]\tLoss: 0.240392\n","Train Epoch: 5 [13000/60000 (22%)]\tLoss: 0.221536\n","Train Epoch: 5 [14000/60000 (23%)]\tLoss: 0.289854\n","Train Epoch: 5 [15000/60000 (25%)]\tLoss: 0.273448\n","Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.294546\n","Train Epoch: 5 [17000/60000 (28%)]\tLoss: 0.225755\n","Train Epoch: 5 [18000/60000 (30%)]\tLoss: 0.348424\n","Train Epoch: 5 [19000/60000 (32%)]\tLoss: 0.245677\n","Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.237138\n","Train Epoch: 5 [21000/60000 (35%)]\tLoss: 0.220772\n","Train Epoch: 5 [22000/60000 (37%)]\tLoss: 0.375581\n","Train Epoch: 5 [23000/60000 (38%)]\tLoss: 0.175823\n","Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.216956\n","Train Epoch: 5 [25000/60000 (42%)]\tLoss: 0.268571\n","Train Epoch: 5 [26000/60000 (43%)]\tLoss: 0.178900\n","Train Epoch: 5 [27000/60000 (45%)]\tLoss: 0.311619\n","Train Epoch: 5 [28000/60000 (47%)]\tLoss: 0.167941\n","Train Epoch: 5 [29000/60000 (48%)]\tLoss: 0.130096\n","Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.348064\n","Train Epoch: 5 [31000/60000 (52%)]\tLoss: 0.154262\n","Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.278380\n","Train Epoch: 5 [33000/60000 (55%)]\tLoss: 0.249529\n","Train Epoch: 5 [34000/60000 (57%)]\tLoss: 0.315944\n","Train Epoch: 5 [35000/60000 (58%)]\tLoss: 0.229136\n","Train Epoch: 5 [36000/60000 (60%)]\tLoss: 0.150227\n","Train Epoch: 5 [37000/60000 (62%)]\tLoss: 0.285327\n","Train Epoch: 5 [38000/60000 (63%)]\tLoss: 0.202669\n","Train Epoch: 5 [39000/60000 (65%)]\tLoss: 0.331328\n","Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.195624\n","Train Epoch: 5 [41000/60000 (68%)]\tLoss: 0.168296\n","Train Epoch: 5 [42000/60000 (70%)]\tLoss: 0.294621\n","Train Epoch: 5 [43000/60000 (72%)]\tLoss: 0.195288\n","Train Epoch: 5 [44000/60000 (73%)]\tLoss: 0.129118\n","Train Epoch: 5 [45000/60000 (75%)]\tLoss: 0.243334\n","Train Epoch: 5 [46000/60000 (77%)]\tLoss: 0.194492\n","Train Epoch: 5 [47000/60000 (78%)]\tLoss: 0.150822\n","Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.201378\n","Train Epoch: 5 [49000/60000 (82%)]\tLoss: 0.211995\n","Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.347336\n","Train Epoch: 5 [51000/60000 (85%)]\tLoss: 0.401662\n","Train Epoch: 5 [52000/60000 (87%)]\tLoss: 0.132709\n","Train Epoch: 5 [53000/60000 (88%)]\tLoss: 0.164570\n","Train Epoch: 5 [54000/60000 (90%)]\tLoss: 0.278665\n","Train Epoch: 5 [55000/60000 (92%)]\tLoss: 0.350745\n","Train Epoch: 5 [56000/60000 (93%)]\tLoss: 0.145831\n","Train Epoch: 5 [57000/60000 (95%)]\tLoss: 0.155018\n","Train Epoch: 5 [58000/60000 (97%)]\tLoss: 0.190094\n","Train Epoch: 5 [59000/60000 (98%)]\tLoss: 0.422339\n","\n","Test set: Average loss: 0.0022, Accuracy: 9365/10000 (94%)\n","\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.111060\n","Train Epoch: 6 [1000/60000 (2%)]\tLoss: 0.160173\n","Train Epoch: 6 [2000/60000 (3%)]\tLoss: 0.153428\n","Train Epoch: 6 [3000/60000 (5%)]\tLoss: 0.167422\n","Train Epoch: 6 [4000/60000 (7%)]\tLoss: 0.140810\n","Train Epoch: 6 [5000/60000 (8%)]\tLoss: 0.166294\n","Train Epoch: 6 [6000/60000 (10%)]\tLoss: 0.157877\n","Train Epoch: 6 [7000/60000 (12%)]\tLoss: 0.350949\n","Train Epoch: 6 [8000/60000 (13%)]\tLoss: 0.200965\n","Train Epoch: 6 [9000/60000 (15%)]\tLoss: 0.139082\n","Train Epoch: 6 [10000/60000 (17%)]\tLoss: 0.260029\n","Train Epoch: 6 [11000/60000 (18%)]\tLoss: 0.511972\n","Train Epoch: 6 [12000/60000 (20%)]\tLoss: 0.189879\n","Train Epoch: 6 [13000/60000 (22%)]\tLoss: 0.197250\n","Train Epoch: 6 [14000/60000 (23%)]\tLoss: 0.320427\n","Train Epoch: 6 [15000/60000 (25%)]\tLoss: 0.130190\n","Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.318241\n","Train Epoch: 6 [17000/60000 (28%)]\tLoss: 0.227621\n","Train Epoch: 6 [18000/60000 (30%)]\tLoss: 0.347127\n","Train Epoch: 6 [19000/60000 (32%)]\tLoss: 0.172100\n","Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.182836\n","Train Epoch: 6 [21000/60000 (35%)]\tLoss: 0.185968\n","Train Epoch: 6 [22000/60000 (37%)]\tLoss: 0.094062\n","Train Epoch: 6 [23000/60000 (38%)]\tLoss: 0.208513\n","Train Epoch: 6 [24000/60000 (40%)]\tLoss: 0.185162\n","Train Epoch: 6 [25000/60000 (42%)]\tLoss: 0.245799\n","Train Epoch: 6 [26000/60000 (43%)]\tLoss: 0.171696\n","Train Epoch: 6 [27000/60000 (45%)]\tLoss: 0.167678\n","Train Epoch: 6 [28000/60000 (47%)]\tLoss: 0.174398\n","Train Epoch: 6 [29000/60000 (48%)]\tLoss: 0.179569\n","Train Epoch: 6 [30000/60000 (50%)]\tLoss: 0.235451\n","Train Epoch: 6 [31000/60000 (52%)]\tLoss: 0.144977\n","Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.285316\n","Train Epoch: 6 [33000/60000 (55%)]\tLoss: 0.119804\n","Train Epoch: 6 [34000/60000 (57%)]\tLoss: 0.251217\n","Train Epoch: 6 [35000/60000 (58%)]\tLoss: 0.137579\n","Train Epoch: 6 [36000/60000 (60%)]\tLoss: 0.212145\n","Train Epoch: 6 [37000/60000 (62%)]\tLoss: 0.113324\n","Train Epoch: 6 [38000/60000 (63%)]\tLoss: 0.224718\n","Train Epoch: 6 [39000/60000 (65%)]\tLoss: 0.194618\n","Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.307982\n","Train Epoch: 6 [41000/60000 (68%)]\tLoss: 0.188375\n","Train Epoch: 6 [42000/60000 (70%)]\tLoss: 0.140870\n","Train Epoch: 6 [43000/60000 (72%)]\tLoss: 0.229113\n","Train Epoch: 6 [44000/60000 (73%)]\tLoss: 0.117395\n","Train Epoch: 6 [45000/60000 (75%)]\tLoss: 0.261196\n","Train Epoch: 6 [46000/60000 (77%)]\tLoss: 0.209852\n","Train Epoch: 6 [47000/60000 (78%)]\tLoss: 0.258356\n","Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.141429\n","Train Epoch: 6 [49000/60000 (82%)]\tLoss: 0.207000\n","Train Epoch: 6 [50000/60000 (83%)]\tLoss: 0.137291\n","Train Epoch: 6 [51000/60000 (85%)]\tLoss: 0.172929\n","Train Epoch: 6 [52000/60000 (87%)]\tLoss: 0.175348\n","Train Epoch: 6 [53000/60000 (88%)]\tLoss: 0.198400\n","Train Epoch: 6 [54000/60000 (90%)]\tLoss: 0.270019\n","Train Epoch: 6 [55000/60000 (92%)]\tLoss: 0.215128\n","Train Epoch: 6 [56000/60000 (93%)]\tLoss: 0.281456\n","Train Epoch: 6 [57000/60000 (95%)]\tLoss: 0.253575\n","Train Epoch: 6 [58000/60000 (97%)]\tLoss: 0.300656\n","Train Epoch: 6 [59000/60000 (98%)]\tLoss: 0.220890\n","\n","Test set: Average loss: 0.0020, Accuracy: 9424/10000 (94%)\n","\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.329106\n","Train Epoch: 7 [1000/60000 (2%)]\tLoss: 0.143114\n","Train Epoch: 7 [2000/60000 (3%)]\tLoss: 0.336509\n","Train Epoch: 7 [3000/60000 (5%)]\tLoss: 0.092085\n","Train Epoch: 7 [4000/60000 (7%)]\tLoss: 0.123926\n","Train Epoch: 7 [5000/60000 (8%)]\tLoss: 0.132760\n","Train Epoch: 7 [6000/60000 (10%)]\tLoss: 0.426048\n","Train Epoch: 7 [7000/60000 (12%)]\tLoss: 0.179514\n","Train Epoch: 7 [8000/60000 (13%)]\tLoss: 0.194347\n","Train Epoch: 7 [9000/60000 (15%)]\tLoss: 0.233770\n","Train Epoch: 7 [10000/60000 (17%)]\tLoss: 0.348230\n","Train Epoch: 7 [11000/60000 (18%)]\tLoss: 0.182924\n","Train Epoch: 7 [12000/60000 (20%)]\tLoss: 0.166476\n","Train Epoch: 7 [13000/60000 (22%)]\tLoss: 0.216463\n","Train Epoch: 7 [14000/60000 (23%)]\tLoss: 0.154747\n","Train Epoch: 7 [15000/60000 (25%)]\tLoss: 0.193849\n","Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.185791\n","Train Epoch: 7 [17000/60000 (28%)]\tLoss: 0.337608\n","Train Epoch: 7 [18000/60000 (30%)]\tLoss: 0.190861\n","Train Epoch: 7 [19000/60000 (32%)]\tLoss: 0.107502\n","Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.226012\n","Train Epoch: 7 [21000/60000 (35%)]\tLoss: 0.070458\n","Train Epoch: 7 [22000/60000 (37%)]\tLoss: 0.232775\n","Train Epoch: 7 [23000/60000 (38%)]\tLoss: 0.205700\n","Train Epoch: 7 [24000/60000 (40%)]\tLoss: 0.105700\n","Train Epoch: 7 [25000/60000 (42%)]\tLoss: 0.311606\n","Train Epoch: 7 [26000/60000 (43%)]\tLoss: 0.239875\n","Train Epoch: 7 [27000/60000 (45%)]\tLoss: 0.240274\n","Train Epoch: 7 [28000/60000 (47%)]\tLoss: 0.147489\n","Train Epoch: 7 [29000/60000 (48%)]\tLoss: 0.245450\n","Train Epoch: 7 [30000/60000 (50%)]\tLoss: 0.152875\n","Train Epoch: 7 [31000/60000 (52%)]\tLoss: 0.111702\n","Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.097400\n","Train Epoch: 7 [33000/60000 (55%)]\tLoss: 0.245468\n","Train Epoch: 7 [34000/60000 (57%)]\tLoss: 0.141898\n","Train Epoch: 7 [35000/60000 (58%)]\tLoss: 0.163262\n","Train Epoch: 7 [36000/60000 (60%)]\tLoss: 0.344764\n","Train Epoch: 7 [37000/60000 (62%)]\tLoss: 0.203244\n","Train Epoch: 7 [38000/60000 (63%)]\tLoss: 0.159719\n","Train Epoch: 7 [39000/60000 (65%)]\tLoss: 0.198151\n","Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.196004\n","Train Epoch: 7 [41000/60000 (68%)]\tLoss: 0.168679\n","Train Epoch: 7 [42000/60000 (70%)]\tLoss: 0.205219\n","Train Epoch: 7 [43000/60000 (72%)]\tLoss: 0.260981\n","Train Epoch: 7 [44000/60000 (73%)]\tLoss: 0.081522\n","Train Epoch: 7 [45000/60000 (75%)]\tLoss: 0.175560\n","Train Epoch: 7 [46000/60000 (77%)]\tLoss: 0.189444\n","Train Epoch: 7 [47000/60000 (78%)]\tLoss: 0.213140\n","Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.135419\n","Train Epoch: 7 [49000/60000 (82%)]\tLoss: 0.143838\n","Train Epoch: 7 [50000/60000 (83%)]\tLoss: 0.163786\n","Train Epoch: 7 [51000/60000 (85%)]\tLoss: 0.135009\n","Train Epoch: 7 [52000/60000 (87%)]\tLoss: 0.183105\n","Train Epoch: 7 [53000/60000 (88%)]\tLoss: 0.164353\n","Train Epoch: 7 [54000/60000 (90%)]\tLoss: 0.162207\n","Train Epoch: 7 [55000/60000 (92%)]\tLoss: 0.218122\n","Train Epoch: 7 [56000/60000 (93%)]\tLoss: 0.268978\n","Train Epoch: 7 [57000/60000 (95%)]\tLoss: 0.255971\n","Train Epoch: 7 [58000/60000 (97%)]\tLoss: 0.121892\n","Train Epoch: 7 [59000/60000 (98%)]\tLoss: 0.129257\n","\n","Test set: Average loss: 0.0019, Accuracy: 9445/10000 (94%)\n","\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.107851\n","Train Epoch: 8 [1000/60000 (2%)]\tLoss: 0.153498\n","Train Epoch: 8 [2000/60000 (3%)]\tLoss: 0.152310\n","Train Epoch: 8 [3000/60000 (5%)]\tLoss: 0.152642\n","Train Epoch: 8 [4000/60000 (7%)]\tLoss: 0.162764\n","Train Epoch: 8 [5000/60000 (8%)]\tLoss: 0.124736\n","Train Epoch: 8 [6000/60000 (10%)]\tLoss: 0.160606\n","Train Epoch: 8 [7000/60000 (12%)]\tLoss: 0.278031\n","Train Epoch: 8 [8000/60000 (13%)]\tLoss: 0.207374\n","Train Epoch: 8 [9000/60000 (15%)]\tLoss: 0.171150\n","Train Epoch: 8 [10000/60000 (17%)]\tLoss: 0.306877\n","Train Epoch: 8 [11000/60000 (18%)]\tLoss: 0.187382\n","Train Epoch: 8 [12000/60000 (20%)]\tLoss: 0.244485\n","Train Epoch: 8 [13000/60000 (22%)]\tLoss: 0.235370\n","Train Epoch: 8 [14000/60000 (23%)]\tLoss: 0.194924\n","Train Epoch: 8 [15000/60000 (25%)]\tLoss: 0.093709\n","Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.181661\n","Train Epoch: 8 [17000/60000 (28%)]\tLoss: 0.104527\n","Train Epoch: 8 [18000/60000 (30%)]\tLoss: 0.156231\n","Train Epoch: 8 [19000/60000 (32%)]\tLoss: 0.171771\n","Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.184029\n","Train Epoch: 8 [21000/60000 (35%)]\tLoss: 0.098554\n","Train Epoch: 8 [22000/60000 (37%)]\tLoss: 0.180811\n","Train Epoch: 8 [23000/60000 (38%)]\tLoss: 0.117491\n","Train Epoch: 8 [24000/60000 (40%)]\tLoss: 0.168774\n","Train Epoch: 8 [25000/60000 (42%)]\tLoss: 0.219184\n","Train Epoch: 8 [26000/60000 (43%)]\tLoss: 0.166257\n","Train Epoch: 8 [27000/60000 (45%)]\tLoss: 0.141824\n","Train Epoch: 8 [28000/60000 (47%)]\tLoss: 0.092300\n","Train Epoch: 8 [29000/60000 (48%)]\tLoss: 0.213308\n","Train Epoch: 8 [30000/60000 (50%)]\tLoss: 0.339083\n","Train Epoch: 8 [31000/60000 (52%)]\tLoss: 0.124068\n","Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.164167\n","Train Epoch: 8 [33000/60000 (55%)]\tLoss: 0.128989\n","Train Epoch: 8 [34000/60000 (57%)]\tLoss: 0.128647\n","Train Epoch: 8 [35000/60000 (58%)]\tLoss: 0.270235\n","Train Epoch: 8 [36000/60000 (60%)]\tLoss: 0.190878\n","Train Epoch: 8 [37000/60000 (62%)]\tLoss: 0.171575\n","Train Epoch: 8 [38000/60000 (63%)]\tLoss: 0.139911\n","Train Epoch: 8 [39000/60000 (65%)]\tLoss: 0.169168\n","Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.164165\n","Train Epoch: 8 [41000/60000 (68%)]\tLoss: 0.259877\n","Train Epoch: 8 [42000/60000 (70%)]\tLoss: 0.237361\n","Train Epoch: 8 [43000/60000 (72%)]\tLoss: 0.152507\n","Train Epoch: 8 [44000/60000 (73%)]\tLoss: 0.184345\n","Train Epoch: 8 [45000/60000 (75%)]\tLoss: 0.152381\n","Train Epoch: 8 [46000/60000 (77%)]\tLoss: 0.090145\n","Train Epoch: 8 [47000/60000 (78%)]\tLoss: 0.168678\n","Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.122790\n","Train Epoch: 8 [49000/60000 (82%)]\tLoss: 0.088793\n","Train Epoch: 8 [50000/60000 (83%)]\tLoss: 0.198263\n","Train Epoch: 8 [51000/60000 (85%)]\tLoss: 0.171339\n","Train Epoch: 8 [52000/60000 (87%)]\tLoss: 0.147110\n","Train Epoch: 8 [53000/60000 (88%)]\tLoss: 0.111570\n","Train Epoch: 8 [54000/60000 (90%)]\tLoss: 0.063813\n","Train Epoch: 8 [55000/60000 (92%)]\tLoss: 0.234594\n","Train Epoch: 8 [56000/60000 (93%)]\tLoss: 0.146299\n","Train Epoch: 8 [57000/60000 (95%)]\tLoss: 0.069877\n","Train Epoch: 8 [58000/60000 (97%)]\tLoss: 0.179923\n","Train Epoch: 8 [59000/60000 (98%)]\tLoss: 0.155474\n","\n","Test set: Average loss: 0.0017, Accuracy: 9506/10000 (95%)\n","\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.177606\n","Train Epoch: 9 [1000/60000 (2%)]\tLoss: 0.213277\n","Train Epoch: 9 [2000/60000 (3%)]\tLoss: 0.079885\n","Train Epoch: 9 [3000/60000 (5%)]\tLoss: 0.127889\n","Train Epoch: 9 [4000/60000 (7%)]\tLoss: 0.227291\n","Train Epoch: 9 [5000/60000 (8%)]\tLoss: 0.114196\n","Train Epoch: 9 [6000/60000 (10%)]\tLoss: 0.126053\n","Train Epoch: 9 [7000/60000 (12%)]\tLoss: 0.090345\n","Train Epoch: 9 [8000/60000 (13%)]\tLoss: 0.061648\n","Train Epoch: 9 [9000/60000 (15%)]\tLoss: 0.096034\n","Train Epoch: 9 [10000/60000 (17%)]\tLoss: 0.159800\n","Train Epoch: 9 [11000/60000 (18%)]\tLoss: 0.267697\n","Train Epoch: 9 [12000/60000 (20%)]\tLoss: 0.134709\n","Train Epoch: 9 [13000/60000 (22%)]\tLoss: 0.183591\n","Train Epoch: 9 [14000/60000 (23%)]\tLoss: 0.204230\n","Train Epoch: 9 [15000/60000 (25%)]\tLoss: 0.107958\n","Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.152244\n","Train Epoch: 9 [17000/60000 (28%)]\tLoss: 0.232731\n","Train Epoch: 9 [18000/60000 (30%)]\tLoss: 0.168795\n","Train Epoch: 9 [19000/60000 (32%)]\tLoss: 0.127627\n","Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.191914\n","Train Epoch: 9 [21000/60000 (35%)]\tLoss: 0.099061\n","Train Epoch: 9 [22000/60000 (37%)]\tLoss: 0.098797\n","Train Epoch: 9 [23000/60000 (38%)]\tLoss: 0.101717\n","Train Epoch: 9 [24000/60000 (40%)]\tLoss: 0.137588\n","Train Epoch: 9 [25000/60000 (42%)]\tLoss: 0.147853\n","Train Epoch: 9 [26000/60000 (43%)]\tLoss: 0.220558\n","Train Epoch: 9 [27000/60000 (45%)]\tLoss: 0.231187\n","Train Epoch: 9 [28000/60000 (47%)]\tLoss: 0.089735\n","Train Epoch: 9 [29000/60000 (48%)]\tLoss: 0.157318\n","Train Epoch: 9 [30000/60000 (50%)]\tLoss: 0.137410\n","Train Epoch: 9 [31000/60000 (52%)]\tLoss: 0.159820\n","Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.165343\n","Train Epoch: 9 [33000/60000 (55%)]\tLoss: 0.103245\n","Train Epoch: 9 [34000/60000 (57%)]\tLoss: 0.179991\n","Train Epoch: 9 [35000/60000 (58%)]\tLoss: 0.167777\n","Train Epoch: 9 [36000/60000 (60%)]\tLoss: 0.138503\n","Train Epoch: 9 [37000/60000 (62%)]\tLoss: 0.074034\n","Train Epoch: 9 [38000/60000 (63%)]\tLoss: 0.170353\n","Train Epoch: 9 [39000/60000 (65%)]\tLoss: 0.233904\n","Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.211207\n","Train Epoch: 9 [41000/60000 (68%)]\tLoss: 0.120652\n","Train Epoch: 9 [42000/60000 (70%)]\tLoss: 0.176311\n","Train Epoch: 9 [43000/60000 (72%)]\tLoss: 0.083990\n","Train Epoch: 9 [44000/60000 (73%)]\tLoss: 0.150043\n","Train Epoch: 9 [45000/60000 (75%)]\tLoss: 0.112960\n","Train Epoch: 9 [46000/60000 (77%)]\tLoss: 0.193178\n","Train Epoch: 9 [47000/60000 (78%)]\tLoss: 0.145985\n","Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.220254\n","Train Epoch: 9 [49000/60000 (82%)]\tLoss: 0.226287\n","Train Epoch: 9 [50000/60000 (83%)]\tLoss: 0.096477\n","Train Epoch: 9 [51000/60000 (85%)]\tLoss: 0.132679\n","Train Epoch: 9 [52000/60000 (87%)]\tLoss: 0.111162\n","Train Epoch: 9 [53000/60000 (88%)]\tLoss: 0.241109\n","Train Epoch: 9 [54000/60000 (90%)]\tLoss: 0.054942\n","Train Epoch: 9 [55000/60000 (92%)]\tLoss: 0.200080\n","Train Epoch: 9 [56000/60000 (93%)]\tLoss: 0.111318\n","Train Epoch: 9 [57000/60000 (95%)]\tLoss: 0.136281\n","Train Epoch: 9 [58000/60000 (97%)]\tLoss: 0.156632\n","Train Epoch: 9 [59000/60000 (98%)]\tLoss: 0.094817\n","\n","Test set: Average loss: 0.0016, Accuracy: 9523/10000 (95%)\n","\n","Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.227085\n","Train Epoch: 10 [1000/60000 (2%)]\tLoss: 0.164207\n","Train Epoch: 10 [2000/60000 (3%)]\tLoss: 0.161594\n","Train Epoch: 10 [3000/60000 (5%)]\tLoss: 0.118904\n","Train Epoch: 10 [4000/60000 (7%)]\tLoss: 0.146499\n","Train Epoch: 10 [5000/60000 (8%)]\tLoss: 0.145440\n","Train Epoch: 10 [6000/60000 (10%)]\tLoss: 0.077749\n","Train Epoch: 10 [7000/60000 (12%)]\tLoss: 0.094940\n","Train Epoch: 10 [8000/60000 (13%)]\tLoss: 0.050651\n","Train Epoch: 10 [9000/60000 (15%)]\tLoss: 0.102008\n","Train Epoch: 10 [10000/60000 (17%)]\tLoss: 0.187233\n","Train Epoch: 10 [11000/60000 (18%)]\tLoss: 0.200427\n","Train Epoch: 10 [12000/60000 (20%)]\tLoss: 0.176623\n","Train Epoch: 10 [13000/60000 (22%)]\tLoss: 0.266787\n","Train Epoch: 10 [14000/60000 (23%)]\tLoss: 0.136892\n","Train Epoch: 10 [15000/60000 (25%)]\tLoss: 0.155605\n","Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.182894\n","Train Epoch: 10 [17000/60000 (28%)]\tLoss: 0.137774\n","Train Epoch: 10 [18000/60000 (30%)]\tLoss: 0.122816\n","Train Epoch: 10 [19000/60000 (32%)]\tLoss: 0.199327\n","Train Epoch: 10 [20000/60000 (33%)]\tLoss: 0.108539\n","Train Epoch: 10 [21000/60000 (35%)]\tLoss: 0.156319\n","Train Epoch: 10 [22000/60000 (37%)]\tLoss: 0.133393\n","Train Epoch: 10 [23000/60000 (38%)]\tLoss: 0.166834\n","Train Epoch: 10 [24000/60000 (40%)]\tLoss: 0.110264\n","Train Epoch: 10 [25000/60000 (42%)]\tLoss: 0.148771\n","Train Epoch: 10 [26000/60000 (43%)]\tLoss: 0.115904\n","Train Epoch: 10 [27000/60000 (45%)]\tLoss: 0.128245\n","Train Epoch: 10 [28000/60000 (47%)]\tLoss: 0.187600\n","Train Epoch: 10 [29000/60000 (48%)]\tLoss: 0.118185\n","Train Epoch: 10 [30000/60000 (50%)]\tLoss: 0.183190\n","Train Epoch: 10 [31000/60000 (52%)]\tLoss: 0.144389\n","Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.184197\n","Train Epoch: 10 [33000/60000 (55%)]\tLoss: 0.111910\n","Train Epoch: 10 [34000/60000 (57%)]\tLoss: 0.118625\n","Train Epoch: 10 [35000/60000 (58%)]\tLoss: 0.084187\n","Train Epoch: 10 [36000/60000 (60%)]\tLoss: 0.133055\n","Train Epoch: 10 [37000/60000 (62%)]\tLoss: 0.193546\n","Train Epoch: 10 [38000/60000 (63%)]\tLoss: 0.156392\n","Train Epoch: 10 [39000/60000 (65%)]\tLoss: 0.145389\n","Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.219685\n","Train Epoch: 10 [41000/60000 (68%)]\tLoss: 0.132011\n","Train Epoch: 10 [42000/60000 (70%)]\tLoss: 0.263424\n","Train Epoch: 10 [43000/60000 (72%)]\tLoss: 0.179201\n","Train Epoch: 10 [44000/60000 (73%)]\tLoss: 0.168963\n","Train Epoch: 10 [45000/60000 (75%)]\tLoss: 0.121980\n","Train Epoch: 10 [46000/60000 (77%)]\tLoss: 0.232907\n","Train Epoch: 10 [47000/60000 (78%)]\tLoss: 0.111015\n","Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.155713\n","Train Epoch: 10 [49000/60000 (82%)]\tLoss: 0.203379\n","Train Epoch: 10 [50000/60000 (83%)]\tLoss: 0.186610\n","Train Epoch: 10 [51000/60000 (85%)]\tLoss: 0.348831\n","Train Epoch: 10 [52000/60000 (87%)]\tLoss: 0.189511\n","Train Epoch: 10 [53000/60000 (88%)]\tLoss: 0.242291\n","Train Epoch: 10 [54000/60000 (90%)]\tLoss: 0.196216\n","Train Epoch: 10 [55000/60000 (92%)]\tLoss: 0.107033\n","Train Epoch: 10 [56000/60000 (93%)]\tLoss: 0.132118\n","Train Epoch: 10 [57000/60000 (95%)]\tLoss: 0.233226\n","Train Epoch: 10 [58000/60000 (97%)]\tLoss: 0.189974\n","Train Epoch: 10 [59000/60000 (98%)]\tLoss: 0.175429\n","\n","Test set: Average loss: 0.0015, Accuracy: 9550/10000 (96%)\n","\n"]}],"source":["for epoch in range(1, args['epochs'] + 1):\n","    train(epoch)\n","    test()\n"]},{"cell_type":"markdown","metadata":{"id":"1OipUVawbYcr"},"source":["## 2. Tasks"]},{"cell_type":"markdown","metadata":{"id":"GA50s29XbYcs"},"source":["### 1. Please use other activation functions, e.g., sigmoid, tanh, and then plot the training loss and testing accuracy.\n","\n","When plotting the training loss, the x-axis is iteration and the y-axis is training loss. When plotting the testing accuracy,  the x-axis is epoch and the y-axis is the training loss."]},{"cell_type":"code","execution_count":49,"metadata":{"id":"SCUF04MYbYcs","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1718129590256,"user_tz":240,"elapsed":3180,"user":{"displayName":"Jia Gapuz","userId":"08065593811390920067"}},"outputId":"8e5967af-6e23-4007-c805-90a9e232a3ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Net(\n","  (fc1): Linear(in_features=784, out_features=256, bias=True)\n","  (fc2): Linear(in_features=256, out_features=128, bias=True)\n","  (fc3): Linear(in_features=128, out_features=64, bias=True)\n","  (fc4): Linear(in_features=64, out_features=32, bias=True)\n","  (fc5): Linear(in_features=32, out_features=16, bias=True)\n","  (fc6): Linear(in_features=16, out_features=10, bias=True)\n",")\n","\n","Test set: Average loss: 0.0002, Accuracy: 14/10000 (0%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 28/10000 (0%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 41/10000 (0%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 57/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 67/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 73/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 84/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 99/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 110/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 126/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 140/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 148/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 159/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 172/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 179/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 185/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 196/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 209/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 224/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 234/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 244/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 252/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 265/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 279/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 287/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 297/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 305/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 318/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 329/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 340/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 352/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 360/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 376/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 384/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 398/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 405/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 421/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 432/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 440/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 450/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 467/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 485/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 495/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 504/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 508/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 522/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 536/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 547/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 557/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 571/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 584/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 593/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 604/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 615/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 627/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 641/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 653/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 663/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 675/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 686/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 696/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 707/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 717/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 726/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 737/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 746/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 763/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 777/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 785/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 795/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 806/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 816/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 828/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 838/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 849/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 859/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 869/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 879/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 893/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 905/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 918/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 927/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 938/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 950/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 962/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 974/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 985/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 996/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1004/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1016/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1030/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1040/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1051/10000 (11%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1063/10000 (11%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1075/10000 (11%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1086/10000 (11%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1100/10000 (11%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1110/10000 (11%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1123/10000 (11%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1135/10000 (11%)\n","\n"]}],"source":["# your code\n","# build an mlp\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","\n","        self.fc1 = nn.Linear(28*28, 256)   # linear layer (784 -> 256)\n","        self.fc2 = nn.Linear(256,128)  # linear layer (256 -> 128)\n","        self.fc3 = nn.Linear(128,64)  # linear layer (128 -> 10)\n","        self.fc4 = nn.Linear(64,32)\n","        self.fc5 = nn.Linear(32,16)\n","        self.fc6 = nn.Linear(16,10)\n","\n","    def forward(self, x):\n","        h0 = x.view(-1,28*28) #input layer\n","        h1 = F.relu(self.fc1(h0)) # hidden layer 1\n","        h2 = F.relu(self.fc2(h1)) # hidden layer 2\n","        h3 = F.sigmoid(self.fc3(h2))\n","        h4 = F.tanh(self.fc4(h3))\n","        h5 = F.tanh(self.fc5(h4))\n","        h6 = self.fc6(h5) # output layer\n","\n","        return h6\n","\n","model = Net()\n","model.cuda() # put the model on GPU\n","print(model)\n","\n","test_loss = 0\n","correct = 0\n","\n","for data, target in test_loader:\n","    data, target = data.cuda(), target.cuda()\n","\n","    output = model(data)\n","    test_loss += criterion(output, target).item()\n","    pred = output.data.max(1, keepdim=True)[1]\n","    correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"j-ThwUJxbYcs"},"source":["### 2. Please use different layers in the model, e.g., 1 layer, 5 layers, 10 layers,  and then plot the training loss and testing accuracy."]},{"cell_type":"code","execution_count":53,"metadata":{"id":"D2fTAFyEbYcs","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1718129760246,"user_tz":240,"elapsed":2871,"user":{"displayName":"Jia Gapuz","userId":"08065593811390920067"}},"outputId":"738fc983-604d-406d-f129-e5e7ce625e61"},"outputs":[{"output_type":"stream","name":"stdout","text":["Net(\n","  (fc1): Linear(in_features=784, out_features=256, bias=True)\n","  (fc6): Linear(in_features=256, out_features=10, bias=True)\n",")\n","\n","Test set: Average loss: 0.0002, Accuracy: 11/10000 (0%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 19/10000 (0%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 26/10000 (0%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 36/10000 (0%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 45/10000 (0%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 52/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 57/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 70/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 76/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 92/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 101/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 109/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 117/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 127/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 133/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 138/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 149/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 153/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 164/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 179/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 184/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 193/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 207/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 214/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 226/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 233/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 242/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 250/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 265/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 271/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 282/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 293/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 306/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 318/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 330/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 339/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 348/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 357/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 365/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 371/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 382/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 387/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 397/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 405/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 415/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 425/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 439/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 445/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 452/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 460/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 472/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 483/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 492/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 501/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 513/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 518/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 522/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 532/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 540/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 553/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 559/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 569/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 586/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 594/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 606/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 618/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 636/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 651/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 657/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 668/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 682/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 694/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 707/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 720/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 735/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 750/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 760/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 772/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 783/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 794/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 801/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 817/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 837/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 857/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 866/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 873/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 881/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 891/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 908/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 926/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 945/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 952/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 971/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 982/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 996/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1003/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1021/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1034/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1048/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1066/10000 (11%)\n","\n"]}],"source":["# your code\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","\n","        self.fc1 = nn.Linear(28*28, 256)   # linear layer (784 -> 256)\n","        self.fc6 = nn.Linear(256,10)\n","\n","    def forward(self, x):\n","        h0 = x.view(-1,28*28) #input layer\n","        h1 = F.relu(self.fc1(h0)) # hidden layer 1\n","        h6 = self.fc6(h1) # output layer\n","\n","        return h6\n","\n","model = Net()\n","model.cuda() # put the model on GPU\n","print(model)\n","\n","test_loss = 0\n","correct = 0\n","\n","for data, target in test_loader:\n","    data, target = data.cuda(), target.cuda()\n","\n","    output = model(data)\n","    test_loss += criterion(output, target).item()\n","    pred = output.data.max(1, keepdim=True)[1]\n","    correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n"]},{"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","\n","        self.fc1 = nn.Linear(28*28, 256)   # linear layer (784 -> 256)\n","        self.fc2 = nn.Linear(256,128)  # linear layer (256 -> 128)\n","        self.fc3 = nn.Linear(128,64)  # linear layer (128 -> 10)\n","        self.fc4 = nn.Linear(64,32)\n","        self.fc5 = nn.Linear(32,16)\n","        self.fc6 = nn.Linear(16,10)\n","\n","    def forward(self, x):\n","        h0 = x.view(-1,28*28) #input layer\n","        h1 = F.relu(self.fc1(h0)) # hidden layer 1\n","        h2 = F.relu(self.fc2(h1)) # hidden layer 2\n","        h3 = F.sigmoid(self.fc3(h2))\n","        h4 = F.tanh(self.fc4(h3))\n","        h5 = F.tanh(self.fc5(h4))\n","        h6 = self.fc6(h5) # output layer\n","\n","        return h6\n","\n","model = Net()\n","model.cuda() # put the model on GPU\n","print(model)\n","\n","test_loss = 0\n","correct = 0\n","\n","for data, target in test_loader:\n","    data, target = data.cuda(), target.cuda()\n","\n","    output = model(data)\n","    test_loss += criterion(output, target).item()\n","    pred = output.data.max(1, keepdim=True)[1]\n","    correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","\n"],"metadata":{"id":"nwDuPQk-g9aX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","\n","        self.fc1 = nn.Linear(28*28, 256)   # linear layer (784 -> 256)\n","        self.fc2 = nn.Linear(256,128)  # linear layer (256 -> 128)\n","        self.fc3 = nn.Linear(128,64)  # linear layer (128 -> 10)\n","        self.fc4 = nn.Linear(64,32)\n","        self.fc5 = nn.Linear(32,16)\n","        self.fc6 = nn.Linear(16,8)\n","        self.fc7 = nn.Linear(8,4)\n","        self.fc8 = nn.Linear(4,16)\n","        self.fc9 = nn.Linear(16,8)\n","        self.fc10 = nn.Linear(8,4)\n","        self.fc11 = nn.Linear(4,10)\n","\n","    def forward(self, x):\n","        h0 = x.view(-1,28*28) #input layer\n","        h1 = F.relu(self.fc1(h0)) # hidden layer 1\n","        h2 = F.relu(self.fc2(h1)) # hidden layer 2\n","        h3 = F.sigmoid(self.fc3(h2))\n","        h4 = F.tanh(self.fc4(h3))\n","        h5 = F.tanh(self.fc5(h4))\n","        h6 = F.tanh(self.fc6(h5))\n","        h7 = F.tanh(self.fc7(h6))\n","        h8 = F.tanh(self.fc8(h7))\n","        h9 = F.tanh(self.fc9(h8))\n","        h10 = F.tanh(self.fc10(h9))\n","        h11 = self.fc11(h10) # output layer\n","\n","        return h11\n","\n","model = Net()\n","model.cuda() # put the model on GPU\n","print(model)\n","\n","test_loss = 0\n","correct = 0\n","\n","for data, target in test_loader:\n","    data, target = data.cuda(), target.cuda()\n","\n","    output = model(data)\n","    test_loss += criterion(output, target).item()\n","    pred = output.data.max(1, keepdim=True)[1]\n","    correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AdGaGD3DhKQc","executionInfo":{"status":"ok","timestamp":1718129997199,"user_tz":240,"elapsed":1847,"user":{"displayName":"Jia Gapuz","userId":"08065593811390920067"}},"outputId":"321e7d5d-847d-43f2-92b5-88b189bb3aa3"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Net(\n","  (fc1): Linear(in_features=784, out_features=256, bias=True)\n","  (fc2): Linear(in_features=256, out_features=128, bias=True)\n","  (fc3): Linear(in_features=128, out_features=64, bias=True)\n","  (fc4): Linear(in_features=64, out_features=32, bias=True)\n","  (fc5): Linear(in_features=32, out_features=16, bias=True)\n","  (fc6): Linear(in_features=16, out_features=8, bias=True)\n","  (fc7): Linear(in_features=8, out_features=4, bias=True)\n","  (fc8): Linear(in_features=4, out_features=16, bias=True)\n","  (fc9): Linear(in_features=16, out_features=8, bias=True)\n","  (fc10): Linear(in_features=8, out_features=4, bias=True)\n","  (fc11): Linear(in_features=4, out_features=10, bias=True)\n",")\n","\n","Test set: Average loss: 0.0002, Accuracy: 11/10000 (0%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 16/10000 (0%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 24/10000 (0%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 35/10000 (0%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 45/10000 (0%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 62/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 73/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 81/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 95/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 107/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 118/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 126/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 137/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 144/10000 (1%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 154/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 162/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 176/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 189/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 198/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 207/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 221/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 228/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 236/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 246/10000 (2%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 254/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 269/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 281/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 292/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 301/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 316/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 327/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 339/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 347/10000 (3%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 352/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 366/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 373/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 379/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 391/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 397/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 408/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 416/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 422/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 432/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 441/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 449/10000 (4%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 460/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 471/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 481/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 491/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 500/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 510/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 518/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 529/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 539/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 547/10000 (5%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 557/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 566/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 578/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 588/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 595/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 608/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 618/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 628/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 639/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 649/10000 (6%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 662/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 670/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 681/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 692/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 702/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 711/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 722/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 735/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 745/10000 (7%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 755/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 765/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 775/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 783/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 793/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 803/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 812/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 822/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 835/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 845/10000 (8%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 858/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 867/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 875/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 885/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 896/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 908/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 917/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 928/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 939/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 948/10000 (9%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 958/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 969/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 980/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 990/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1000/10000 (10%)\n","\n","\n","Test set: Average loss: 0.0002, Accuracy: 1010/10000 (10%)\n","\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":0}